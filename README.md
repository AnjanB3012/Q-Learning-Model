# Q-Learning-Model
Flappy Bird Reinforcement Learning (Q-Learning & Beyond)
ğŸš€ An RL-based Flappy Bird agent that learns to survive by optimizing its movements using Q-learning.
Currently trained using a Q-table, with plans to migrate to DQN for better scalability.

ğŸ“Œ Project Overview
This project uses Q-learning to train an agent to play Flappy Bird autonomously.
The goal is to make the model:
âœ… Learn from past experiences
âœ… Maximize long-term survival
âœ… Handle difficult scenarios like extreme pillar gaps
âœ… Improve training efficiency and scalability

Current Features
âœ… Q-learning implementation with a lookup table
âœ… Experience replay stored in XML
âœ… Dynamic reward function based on survival and obstacle clearance

ğŸ“Š Challenges & Setbacks
1ï¸âƒ£ Incorrect State Proximity Calculation ğŸ§©
ğŸ”´ Problem: The agent mistakenly learned from the farthest Q-value instead of the closest, leading to incorrect updates.
âœ… Fix: Corrected the proximity function to select the nearest state.

2ï¸âƒ£ Over-Penalizing Early Training Phases â›”
ğŸ”´ Problem: The agent received 10x harsher punishment in early training, making it too risk-averse.
âœ… Fix: Standardized reward scaling across all training phases.

3ï¸âƒ£ Agent Struggled with Extreme Gaps ğŸ¯
ğŸ”´ Problem: The agent performed well on small pillar variations but failed when gaps were drastically high/low.
âœ… Fix: Improved state representation by adding bird velocity and increasing discount factor (Î³) for better long-term planning.

ğŸ”œ Next Steps
ğŸ”¹ Optimize training efficiency by introducing multi-threading.
ğŸ”¹ Implement CUDA-based Q-table updates for faster training.
ğŸ”¹ Migrate from Q-table to Deep Q-Networks (DQN) for scalability.
ğŸ”¹ Improve generalization to handle unseen pillar variations better.

ğŸ“ Lessons Learned
âœ… Reinforcement Learning requires deep debuggingâ€”mistakes often go unnoticed until training breaks down.
âœ… Reward function tuning is criticalâ€”wrong scaling can ruin learning stability.
âœ… Q-learning struggles with large state-action spaces, requiring optimization techniques like shared instances and function approximation.